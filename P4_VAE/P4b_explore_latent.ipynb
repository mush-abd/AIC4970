{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a56c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulate_latent_space.py\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.utils import save_image\n",
    "from train_vae import VAE, IMG_SIZE, LATENT_DIM, DEVICE  # Import from the training script\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = 'models/vae_celeba_final.pth' # Path to your trained model\n",
    "ATTRIBUTES_FILE = 'data/celeba/list_attr_celeba.csv'\n",
    "IMAGES_DIR = 'data/celeba/img_align_celeba/img_align_celeba/'\n",
    "N_IMAGES_FOR_VECTOR = 5000 # Number of images to average for attribute vectors\n",
    "N_TRANSITIONS = 11 # Number of steps in the generated sequence\n",
    "MANIPULATION_STRENGTH = 3.0 # How strongly to apply the attribute\n",
    "\n",
    "# --- Load Model ---\n",
    "model = VAE().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "# --- Load Attribute Data ---\n",
    "attributes = pd.read_csv(ATTRIBUTES_FILE)\n",
    "# CelebA labels are -1 (absent) and 1 (present). Convert to 0 and 1.\n",
    "attributes.replace(-1, 0, inplace=True)\n",
    "\n",
    "# --- Function to get latent vectors for attribute ---\n",
    "def get_attribute_vectors(attr_name):\n",
    "    # Get image filenames for positive and negative samples\n",
    "    positive_files = attributes[attributes[attr_name] == 1]['image_id'].tolist()\n",
    "    negative_files = attributes[attributes[attr_name] == 0]['image_id'].tolist()\n",
    "    \n",
    "    # Simple transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    def get_mean_latent_vector(filenames, n_samples):\n",
    "        vectors = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(min(n_samples, len(filenames))):\n",
    "                img_path = os.path.join(IMAGES_DIR, filenames[i])\n",
    "                image = transform(Image.open(img_path)).unsqueeze(0).to(DEVICE)\n",
    "                mu, _ = model.encode(image)\n",
    "                vectors.append(mu.cpu().numpy())\n",
    "        return np.mean(np.array(vectors), axis=0).squeeze()\n",
    "\n",
    "    print(f\"Calculating vector for '{attr_name}'...\")\n",
    "    positive_vec = get_mean_latent_vector(positive_files, N_IMAGES_FOR_VECTOR)\n",
    "    negative_vec = get_mean_latent_vector(negative_files, N_IMAGES_FOR_VECTOR)\n",
    "    \n",
    "    # The attribute vector is the difference between the means\n",
    "    attribute_vector = positive_vec - negative_vec\n",
    "    return torch.from_numpy(attribute_vector).float().to(DEVICE)\n",
    "\n",
    "\n",
    "# --- Generate Image Sequence ---\n",
    "def generate_transition(start_img_path, attribute_vector, attribute_name):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(IMG_SIZE),\n",
    "        transforms.CenterCrop(IMG_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Get latent vector of the starting image\n",
    "    start_image = transform(Image.open(start_img_path)).unsqueeze(0).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        start_mu, _ = model.encode(start_image)\n",
    "\n",
    "    # Generate sequence\n",
    "    images = []\n",
    "    alphas = np.linspace(-MANIPULATION_STRENGTH, MANIPULATION_STRENGTH, N_TRANSITIONS)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for alpha in alphas:\n",
    "            # Modify the latent vector\n",
    "            new_z = start_mu + alpha * attribute_vector\n",
    "            generated_img = model.decode(new_z)\n",
    "            images.append(generated_img.cpu())\n",
    "    \n",
    "    # Save the grid\n",
    "    output = torch.cat(images)\n",
    "    save_image(output, f'results/transition_{attribute_name}.png', nrow=N_TRANSITIONS)\n",
    "    print(f\"Saved image grid for '{attribute_name}' transition.\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    from PIL import Image\n",
    "    import os\n",
    "    from torchvision import transforms\n",
    "\n",
    "    # Find a neutral starting image (e.g., not smiling, no eyeglasses)\n",
    "    neutral_candidates = attributes[(attributes['Smiling'] == 0) & (attributes['Eyeglasses'] == 0) & (attributes['Blond_Hair'] == 0)]\n",
    "    start_image_file = neutral_candidates.iloc[10]['image_id'] # Pick an arbitrary one\n",
    "    start_image_path = os.path.join(IMAGES_DIR, start_image_file)\n",
    "\n",
    "    # 1. Smiling\n",
    "    smiling_vector = get_attribute_vectors('Smiling')\n",
    "    generate_transition(start_image_path, smiling_vector, 'Smiling')\n",
    "    \n",
    "    # 2. Eye Openness (using 'Eyeglasses' as a strong proxy that affects the eye region)\n",
    "    eyeglasses_vector = get_attribute_vectors('Eyeglasses')\n",
    "    generate_transition(start_image_path, eyeglasses_vector, 'Eyeglasses')\n",
    "\n",
    "    # 3. Hairstyle (using 'Blond_Hair' as an example)\n",
    "    blond_vector = get_attribute_vectors('Blond_Hair')\n",
    "    generate_transition(start_image_path, blond_vector, 'Blond_Hair')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
